# -*- coding: utf-8 -*-
"""
Created on Mon Mar 29 00:40:35 2021

@author: Nico
"""

#########
## NN  ##
#########

import torch
import torch.optim as optim
import torch.nn as nn
from torch.autograd import variable
import torch.nn.functional as F
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

df4 = pd.read_csv(r'C:\Users\Nico\Documents\Data\covid_df4.csv')
df4.dtypes

del df4['Unnamed: 0']

X_p = np.array(df4.drop('died', axis = 1))
y_p = df4['died'].values

X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(X_p, y_p, test_size=0.3, random_state=30)

## defining the net 

dim_in = X_train_p.shape[1]

## imp: one GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


class MeinNetz(nn.Module)  :# erbt von module 
    def __init__(self):
        super(MeinNetz, self).__init__()
        # layer
        self.lin1 = nn.Linear(dim_in, 10) 
        self.lin2 = nn.Linear(10,20)  
        self.lin3 = nn.Linear(20,20)  
        self.lin4 = nn.Linear(20,10)
        self.lin5 = nn.Linear(10,1)
        
    def forward(self, x):
        # forward pass hier - backprop später
        x = F.tanh(self.lin1(x))
        x = F.tanh(self.lin2(x))
        x = F.tanh(self.lin3(x))
        x = F.tanh(self.lin4(x))
        x = torch.sigmoid(self.lin5(x))  # letzte ohen actiovation
        return x
        
    def num_flat_features(self, x):
        # für 
        size = x.size()[1:]      # richtige batch dim hier definieren 
        num = 1
        
        for i in size:
            num *= i  # 
        return num

netz = MeinNetz()
    
print(netz)  # structure    

###
import os

if os.path.isfile('Netz1.pt'):
    netz = torch.load('Netz1.pt')
###
    
### Calling the NN  ###
   
   
criterion = torch.nn.BCELoss()

learning_rate = 1e-4

optimizer = torch.optim.Adam(netz.parameters(), lr=learning_rate)

train_x = torch.tensor(X_train_p)
train_y = torch.tensor(y_train_p)

##

l_iterations = []
l_loss = []
    
##
for i in range(1000):
    out = netz(train_x.float())
     
    loss = criterion(out, train_y.float())
    
    if i % 100 == 99:
        print(i, loss.item())
        
    l_iterations.append(i)
    l_loss.append(loss)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# resaving
if os.path.isfile('Netz1.pt'):
    netz = torch.load('Netz1.pt')
    
# visualizing loss

plt.figure()
plt.plot(l_iterations, l_loss)
plt.xlabel("# Iterations")
plt.ylabel("Training loss")
# nice 

# final loss


# predicting 
test_x = torch.tensor(X_test_p)
test_y = torch.tensor(y_test_p)

out_test = netz(test_x.float()) 
loss = criterion(out_test, test_y.float())
